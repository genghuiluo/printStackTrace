{"./":{"url":"./","title":"About This Handout","keywords":"","body":"printStackTrace "},"spark/":{"url":"spark/","title":"Learning Spark","keywords":"","body":"Reference Links Spark lastest version doc: http://spark.apache.org/docs/latest/. For historical versions, find here. Github repo of Apache Spark (ASF, Apache Software Foundation): https://github.com/apache/spark . Contribute it with your PR! Submit a bug at https://issues.apache.org/jira/projects/SPARK/summary . 《Learning Spark》 What is Spark ? Spark is a ___ ? Currently (these definitions are changable in developer community and updated overtime) we said ... Spark is a unified analytics engine for large-scale data processing. 关于 是什么的定义(what-is) 时常随着开源项目本身的发展 PMC Member 们会进行讨论、调整(下一个Major Releasae), 简单来说, Spark是一个分布式计算框架(引擎), 很多开发者常喜欢用它和MapReduce做比较. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs(DAG, Directed Acyclic Graph) for data analysis. 高阶 API 又可以划分为 RDD / Dataset (Dataframe, Spark>=2.0之后 Dataframe API 和 Dataset 进行了合并) 2大类, 后者是社区参考著名的 Python 包 Pandas 的 API 设计风格(更利于分析师使用) 二次封装 RDD API 实现 It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing. 除了可以直接使用上面提到的 RDD/Dataset API 开发 Spark Applicaiton 之外. Spark 还提供针对特定场景的内嵌库(built-in library). Structured Dataset Query & SQL -- Spark SQL 在构建现代化数仓中相当流行 Structured Streaming -- Spark Streaming 由于 Spark2.0 后 Dataset API 的统一, 使得Spark Streaming可以受利于Spark SQL GraphX 图计算 MLlib 机器学习 Install Spark Download spark via https://spark.apache.org/downloads.html . Enusre you have JDK(>=1.8) installed yet first (java -version). Since Spark uses Hadoop’s client libraries for HDFS and YARN, you could : download a pr-built spark(aka binary version) with a specified hadoop version, e.g. Chose a package type \"Pre-built for Apache Hadoop 2.7 and later\" download a pr-built spark only (choose \"Pre-built with user-provided Apache Hadoop\") and setup spark CLASSPATH like below. # download any hadoop binary version (e.g. 2.8.2) and setup Pseudo-Distributed 单机伪分布 # setup spark CLASSPATH vi conf/spark-env.sh export SPARK_DIST_CLASSPATH=$(hadoop classpath) download spark source code (or clone from github), then build it with specified hadoop dependency For version sepecific, YOU SHOULD KNOW : Spark runs on Java 8, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.4.4 uses Scala 2.12. You will need to use a compatible Scala version (2.12.x). Support for Java 7, Python 2.6 and old Hadoop versions before 2.6.5 were removed as of Spark 2.2.0. Support for Scala 2.10 was removed as of 2.3.0. Support for Scala 2.11 is deprecated as of Spark 2.4.1 and will be removed in Spark 3.0. Run a Example Locally More details refer to https://spark.apache.org/docs/latest/submitting-applications.html . $ cd spark-2.4.4-bin-hadoop2.7 $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\ --master local[2] \\ examples/jars/spark-examples_2.11-2.4.4.jar 10 # --class 指定 spark app 的入口类, 必须包含 main 方法 # --master 指定 master URL, local[2] 表示 本地执行 & 2个worker线程 # examples/xxx.jar 指定 jar包 路径 # 10 入口类 main 方法接受的参数 String[] args # 这是一个计算圆周率的实例，下面是部分STDOUT打印出来的日志 ... 19/10/09 15:26:06 INFO SparkContext: Submitted application: Spark Pi ... 19/10/09 15:26:07 INFO Utils: Successfully started service 'sparkDriver' on port 63772. 19/10/09 15:26:07 INFO SparkEnv: Registering MapOutputTracker 19/10/09 15:26:07 INFO SparkEnv: Registering BlockManagerMaster ... 19/10/09 15:26:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy ... 19/10/09 15:26:08 INFO SparkContext: Starting job: reduce at SparkPi.scala:38 19/10/09 15:26:08 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 2 output partitions 19/10/09 15:26:08 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38) ... 19/10/09 15:26:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks 19/10/09 15:26:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7866 bytes) 19/10/09 15:26:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7866 bytes) 19/10/09 15:26:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0) 19/10/09 15:26:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1) 19/10/09 15:26:08 INFO Executor: Fetching spark://lm-shb-24502336.paypalcorp.com:63772/jars/spark-examples_2.11-2.4.4.jar with timestamp 1570605967610 ... 19/10/09 15:26:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 824 bytes result sent to driver 19/10/09 15:26:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 292 ms on localhost (executor driver) (1/2) 19/10/09 15:26:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 274 ms on localhost (executor driver) (2/2) 19/10/09 15:26:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 19/10/09 15:26:08 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.538 s 19/10/09 15:26:08 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.600470 s Pi is roughly 3.1421791421791423 ... 19/10/09 15:26:08 INFO MemoryStore: MemoryStore cleared 19/10/09 15:26:08 INFO BlockManager: BlockManager stopped 19/10/09 15:26:08 INFO BlockManagerMaster: BlockManagerMaster stopped 19/10/09 15:26:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped! 19/10/09 15:26:08 INFO SparkContext: Successfully stopped SparkContext 19/10/09 15:26:08 INFO ShutdownHookManager: Shutdown hook called Next section, we will go through the inner process of a spark applicaiton after submitted. "},"spark/debug_in_idea.html":{"url":"spark/debug_in_idea.html","title":"Debug in IDEA","keywords":"","body":"Debug in IDEA We will submit SparkPi to simple Spark Standalone Cluster(one master + one worker) in client deploy mode. Utilize IDEA Remote debugger, we will go through the lifecycle of a spark applicaiton. Before we start, let's copy the original words from 《Learning Spark 2015》- CHAPTER 7 \"Running on a Cluster\". To summarize the concepts in this section, let’s walk through the exact steps that occur when you run a Spark application on a cluster: The user submits an application using spark-submit. spark-submit launches the driver program and invokes the main() method specified by the user. The driver program contacts the cluster manager to ask for resources to launch executors. The cluster manager launches executors on behalf of the driver program. The driver process runs through the user application. Based on the RDD actions and transformations in the program, the driver sends work to executors in the form of tasks. Tasks are run on executor processes to compute and save results. If the driver’s main() method exits or it calls SparkContext.stop(), it will termi‐ nate the executors and release resources from the cluster manager. Step1: clone source code and import into IDEA $ git clone git@github.com:apache/spark.git spark-source-code $ cd spark-source-code # default is master brach which is 3.0.0 version, checkout to branch-2.4 $ git checkout --track origin/branch-2.4 # Open IDEA => Import project from existed source Spark is mainly written in Scala and you use different build tools like maven/sbt/gradle, we will use maven here. Wait for IDEA downloaded all dependencies via maven ... Step2: configure IDEA Debugger Then add four Remote Debugger like below (Run => Edit Configuration). You should set different and avaliable(not in bind already) ports for Debuggers to listen (default 5005). In our cases, MasterDebugger => 5005 WorkerDebugger => 5006 DriverDebugger => 5007 ExecutorDebugger => 5008 Next, copy conf/spark-env.sh.template to a new environment variable configure file conf/spark-env.sh and add lines below to enable these Debuggers. # conf/spark-env.sh SPARK_MASTER_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 -Dspark.master.rest.enabled=true\" SPARK_WORKER_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5006\" SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5007\" Step3: start a simple Spark Standalone Cluster $ sbin/start-all.sh starting org.apache.spark.deploy.master.Master, logging to /xxx/spark-2.4.4-bin-hadoop2.7/logs/spark-geluo-org.apache.spark.deploy.master.xxx.out localhost: starting org.apache.spark.deploy.worker.Worker, logging to /xxx/spark-2.4.4-bin-hadoop2.7/logs/spark-geluo-org.apache.spark.deploy.worker.Worker-1-xxx.out # access web UI of Spark Master at http://localhost:8080 Step4: submit SparkPi Start MasterDebugger and WorkerDebugger in IDEA. $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\ --master spark://localhost:7077 \\ --conf spark.executor.extraJavaOptions=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5008\" \\ --executor-memory=2G --total-executor-cores=2 \\ examples/jars/spark-examples_2.11-2.4.4.jar 20 Listening for transport dt_socket at address: 5007 # spark://localhost:7077 is the default master url(traditional RPC gateway) of Spark Master # --deploy-mode default is client # since we configure \"suspend=y\" only for DriverDebugger JVM parameter above, this app wouldn't lanuch until DriverDebugger started bin/spark-submit and sbin/start-all.sh will both call bin/spark-class inside lanuncher => java => Main is the entrance class used inside bin/spark-class to build cmd string. 可以发现 conf/spark-env.sh 中设定的2个环境变量, SPARK_MASTER_OPTS & SPARK_WORKER_OPTS 是生效的. For Spark Master, java -cp 'xxx' -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 -Dspark.master.rest.enabled=true -Xmx1g org.apache.spark.deploy.master.Master --host LM-SHB-24502336 --port 7077 --webui-port 8080. For Spark Worker, java -cp 'xxx' -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5006 -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://LM-SHB-24502336:7077. SPARK_EXECUTOR_OPTS 是无效的环境变量, 虽然它依旧保留在 SparkClassCommandBuilder switch-case 分支中，但是 executor 进程的启动不是通过 bin/spark-class, 如下 Work 日志中所示 . 19/10/10 21:56:05 INFO ExecutorRunner: Launch command: \"java\" \"-cp\" \"/xxx/spark-2.4.4-bin-hadoop2.7/conf/:/xxx/spark-2.4.4-bin-hadoop2.7/jars/*\" \"-Xmx2048M\" \"-Dspark.driver.port=59711\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"--driver-url\" \"spark://CoarseGrainedScheduler@192.168.0.7:59711\" \"--executor-id\" \"0\" \"--hostname\" \"192.168.0.7\" \"--cores\" \"2\" \"--app-id\" \"app-20191010215605-0000\" \"--worker-url\" \"spark://Worker@192.168.0.7:59617\" For bin/spark-submit, use SparkSubmitCommandBuilder instead of SparkClassCommandBuilder here. Build cmd result looks like: java -cp 'xxx' -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5007 -Xmx1g org.apache.spark.deploy.SparkSubmit # same arguments which we pass to spark-submit above --master spark://LM-SHB-24502336:7077 --conf spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5008 --class org.apache.spark.examples.SparkPi --executor-memory 2G --total-executor-cores 2 examples/jars/spark-examples_2.11-2.4.4.jar 20`. # SPARK_SUBMIT_OPTS works here too. Step5: Ready ! You can set breakpoints in IDEA and start to debug source code of Spark now. core => scala => deploy => SparkSubmit is the entrance class of spark-submit. // line 929 submit.doSubmit(args) // line 76 def doSubmit(args: Array[String]): Unit = { // 构造 SparkSubmitArguments 对象 ( 其过程中会进行参数验证 validateArguments() 等 ) val appArgs = parseArguments(args) appArgs.action match { case SparkSubmitAction.SUBMIT => submit(appArgs, uninitLog) case SparkSubmitAction.KILL => kill(appArgs) case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs) case SparkSubmitAction.PRINT_VERSION => printVersion() } // 默认为 SUBMIT action 即正常提交 // --kill 和 --status 分别对应 KILL & REQUEST_STATUS action, 仅在 standalone + cluster mode 下生效, 并且后面的参数为submitted driver id(如下)而不是常规的application id // 19/10/15 12:46:41 INFO ClientEndpoint: Driver successfully submitted as driver-20191015124641-0000 // --version 对应 PRINT_VERSION action 调用 printVersion() 方法 // 当 没有参数 或 --help 或 无效参数时，都会调用 SparkSubmitArguments 的 printUsageAndExit() 方法打印用例说明并退出 // line 773 private def runMain(args: SparkSubmitArguments, uninitLog: Boolean): Unit = { // prepareSubmitEnvironment 其实做了很多事情 (比如, 检查指定的cluster manager和deploy mode是否为支持的组合, e.g. spark-shel 和 spark-sql 就不支持 cluster mode) // 虽然它最终只返回: // 1. Driver 的参数, e.g. 20 // 2. Driver 的 classpath, e.g. /xxx/examples/jars/spark-examples_2.11-2.4.4.jar // 3. 从参数和全局的 properties 中获取并构造SparkConf(这是下一步构造SparkContext的必要参数) // 4. 包含 main 方法的入口类名 (如果是python application 或者cluster deploy mode，那该类名不同步--class所指定的), // e.g. standalone cluster mode 这里的类名将是 org.apache.spark.deploy.ClientApp // e.g. yarn cluster mode 这里的类名将是 org.apache.spark.deploy.yarn.YarnClusterApplication val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args) // 利用反射调用 入口类(child class) 的 main 方法 app.start(childArgs.toArray, sparkConf) // core => scala => deploy => SparkApplication // line 52 val sysProps = conf.getAll.toMap sysProps.foreach { case (k, v) => sys.props(k) = v } //利用 JVM System Property 传递配置项 e.g spark.master = spark://localhost:7077 mainMethod.invoke(null, args) examples => scala => SparkPi, invoke main function of Driver program. object SparkPi { def main(args: Array[String]) { val spark = SparkSession .builder .appName(\"Spark Pi\") .getOrCreate() // 构造 SparkSession, The entry point to programming Spark with the Dataset and DataFrame API. // parallelize 创建了一个 RDD 经过 map (转换--transformation) 和 reduce (动作--action) val count = spark.sparkContext.parallelize(1 until n, slices).map { i => val x = random * 2 - 1 val y = random * 2 - 1 if (x*x + y*y sql => core => scala => SparkSession // line 901 def getOrCreate(): SparkSession = synchronized { // line 926 val sparkContext = userSuppliedContext.getOrElse { // line 935 // SparkSession 包含一个 SparkContext SparkContext.getOrCreate(sparkConf) // line 957 session = new SparkSession(sparkContext, None, None, extensions) ... // 回到上面的 Driver 程序 core => scala => SparkContext // line 2520 // 如果找不到一个 active context (最多只有一个), 则构造一个新的 Spark Context if (activeContext.get() == null) { setActiveContext(new SparkContext(config), allowMultipleContexts = false) // line 424 // 构造 SparkEnv, Holds all the runtime environment objects for a running Spark instance (either master or worker), // including the serializer, RpcEnv, block manager, map output tracker, etc. _env = createSparkEnv(_conf, isLocal, listenerBus) // line 452 // 启动 WebUI _ui.foreach(_.bind()) // 19/10/15 13:47:14 INFO Utils: Successfully started service 'SparkUI' on port 4040. // 19/10/15 13:47:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lm-shb-24502336.paypalcorp.com:4040 // line 493 // 构造 ScheduleBackend & TaskScheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode) // 构造 DAGScheduler _dagScheduler = new DAGScheduler(this) // 启动 TaskScheduler & ScheduleBackend (TaskScheduler包含一个ScheduleBackend和DAGScheduler的引用--reference) _taskScheduler.start() // 启动之后, Spark Master Web UI http://localhost:8088 Running Application 可以看到 SparkPi 的 RUNNING State 的记录 // BlockManager _env.blockManager.initialize(_applicationId) // MetricSystem _env.metricsSystem.start() // Dynamic Allocation val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) // 未开启 None _executorAllocationManager.foreach(_.start()) ... // SparkContext 构造完了, 回到上面的 SparkSession 构造过程 "},"spark/rdd.html":{"url":"spark/rdd.html","title":"RDD programming","keywords":"","body":""},"spark/dataframe.html":{"url":"spark/dataframe.html","title":"Dataframe","keywords":"","body":""},"spark/dataset.html":{"url":"spark/dataset.html","title":"Dataset","keywords":"","body":""},"hadoop/":{"url":"hadoop/","title":"Learning Hadoop","keywords":"","body":""}}